{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10115399,"sourceType":"datasetVersion","datasetId":6241012}],"dockerImageVersionId":30804,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.feature_selection import mutual_info_classif\n\n# 1. Загрузка данных\ndf = pd.read_csv('/kaggle/input/spotify/dataset (2).csv')\n\n# Предварительная обработка данных (замена NaN, проверка типов)\nprint(\"Первые 5 строк датасета:\")\nprint(df.head())\nprint(\"\\nОписание данных:\")\nprint(df.info())\n\n# Разделение признаков и целевой переменной\ntarget_column = 'popularity'\ny = df[target_column]\nX = df.drop(columns=[target_column])\n\n# Обработка пропусков\nfor col in X.columns:\n    if X[col].dtype == 'object':\n        X[col] = pd.to_numeric(X[col], errors='coerce')\nX.fillna(X.median(numeric_only=True), inplace=True)\n\n# 2. EDA (исследовательский анализ)\nsns.set(style=\"whitegrid\")\n\n# 2.1 Распределение целевой переменной\nplt.figure(figsize=(8, 4))\nsns.countplot(y=y, palette='viridis')\nplt.title('Распределение целевой переменной')\nplt.show()\n\n# Вывод: смотрим на дисбаланс классов\nprint(\"\\nРаспределение целевой переменной:\")\nprint(y.value_counts())\n\n# 2.2 Корреляция числовых переменных\nplt.figure(figsize=(12, 8))\ncorrelation_matrix = X.corr()\nsns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')\nplt.title('Корреляция признаков')\nplt.show()\n\n# Вывод: Выбираем признаки с высокой корреляцией (по модулю > 0.7) и решаем об их удалении\n\n# 2.3 Анализ выбросов\nfor col in X.select_dtypes(include=[np.number]).columns:\n    plt.figure(figsize=(8, 4))\n    sns.boxplot(data=X, x=col, color='salmon')\n    plt.title(f'Анализ выбросов для {col}')\n    plt.show()\n\n# Вывод: Проверяем необходимость обработки выбросов (например, логарифмирование или Winsorization)\n\n# 3. Feature Engineering\n# Пример: создание нового признака, произведение двух числовых колонок\nif len(X.select_dtypes(include=[np.number]).columns) >= 2:\n    X['new_feature'] = X.iloc[:, 0] * X.iloc[:, 1]\n    print(\"Создан новый признак: произведение двух первых числовых колонок\")\n\n# 3.1 Корреляция новых признаков с таргетом\nnew_corr = pd.Series(mutual_info_classif(X, y, discrete_features=False), index=X.columns)\nprint(\"Корреляция признаков с таргетом:\")\nprint(new_corr.sort_values(ascending=False))\n\n# 4. Feature Importances (Random Forest)\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X, y)\nfeature_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeature_importances.sort_values(ascending=False).plot(kind='bar', figsize=(10, 5))\nplt.title('Важность признаков (Random Forest)')\nplt.show()\n\n# 5. Эксперименты с моделями машинного обучения\n# Разделение данных на тренировочные и тестовые\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Нормализация данных\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 5.1 Линейная модель\nlogreg = LogisticRegression(max_iter=1000, random_state=42)\nlogreg.fit(X_train_scaled, y_train)\ny_pred_logreg = logreg.predict(X_test_scaled)\nprint(\"Logistic Regression Report:\")\nprint(classification_report(y_test, y_pred_logreg))\n\n# 5.2 Модель на основе деревьев\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\nprint(\"Random Forest Report:\")\nprint(classification_report(y_test, y_pred_rf))\n\n# 5.3 Градиентный бустинг\nxgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\nxgb.fit(X_train, y_train)\ny_pred_xgb = xgb.predict(X_test)\nprint(\"XGBoost Report:\")\nprint(classification_report(y_test, y_pred_xgb))\n\n# 5.4 Нейронная сеть\nmlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\nmlp.fit(X_train_scaled, y_train)\ny_pred_mlp = mlp.predict(X_test_scaled)\nprint(\"MLP Report:\")\nprint(classification_report(y_test, y_pred_mlp))\n\n# 6. Кросс-валидация для лучшей модели\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(xgb, X, y, cv=kf, scoring='accuracy')\nprint(\"Средняя точность модели XGBoost на кросс-валидации:\", scores.mean())\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T04:31:52.984073Z","iopub.execute_input":"2024-12-06T04:31:52.984465Z","iopub.status.idle":"2024-12-06T04:32:01.162761Z","shell.execute_reply.started":"2024-12-06T04:31:52.984433Z","shell.execute_reply":"2024-12-06T04:32:01.161171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"К сожелению я так и не разобрался с Kaggle, и как грамотно разбить на участки кода и оставить грамотные комментарии. Краткие комментраии присутствуют в коде, также разбил комментариями код для лучшего понимания. ","metadata":{}},{"cell_type":"markdown","source":"Общий вывод по работе\nПроведя анализ данных, я понял, что на популярность трека влияют в основном танцевальность (пригодность трека под танцы (danceability)), энергичность (energy) (на сколько динамичен трек), loudness (громкость трека), а также темп трека. С помощью модели случайного леса, выяснено, что танцевальность и энергичность трека наиболее значимые для популярности. \nТакже был создан новый признак, значимость которых невысока, по сравнению с уже имеющимеся.","metadata":{}},{"cell_type":"markdown","source":"Результаты моделей\nВ работе были использованы четыре типа моделей: XGBoost, Random forest, Логическая регрессия и нейронная сеть. Наиболее высокую точность и стабильность предсказаний показала модель XGBoost.\nRandom forest также выделили основные признаки популярности трека, подтвердив выводы XGBoost.\nЛог. регрессия справилась хуже, из-за сложных зависимостей между популярностью и их признаками.\nНейронная сеть также выдаха хорошие результаты, но требует больше ресурсов для ее обучения.","metadata":{}},{"cell_type":"markdown","source":"Исходя из всего перечисленного, основное влияние на популярность трека оказываеть танцевательность и энергичность трека. Темп и длительность трека имеют менее значительное влияние на популярность.\nКод: код неидеален, имеет баги.\n","metadata":{}}]}